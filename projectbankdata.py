# -*- coding: utf-8 -*-
"""ProjectBankData

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QFwCZR5blWP5D5kObQ5U9MMYmxMnNMvU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
##pandas for data manupulation,numpy for numical calculation,matplotlib for data visualisation,seaborn for advanced visualisation

bankdata=pd.read_csv("/content/drive/MyDrive/Project Data.csv")  #load the data

bankdata.shape  #getting shape of data (85585,18)

bankdata.isna().sum()  ##entire column of sub-issue is having null values,Consumer complaint narrative,Tags,Consumer consent provided? columns are also having a lot of null values

bankdata.duplicated().sum()  ##0 duplicates are present

bankdata.describe()  ##no need of knowing statistical values since no proper numerical data present

bankdata.info()  #almost all the columns are object type

#since sub-issues column has no values,we can delete that
bankdata.drop(['Sub-issue','State','Submitted via','Company response to consumer','Timely response?','Consumer disputed?','Tags', 'Company public response','Consumer complaint narrative','Consumer consent provided?'],axis=1,inplace=True)

bankdata.columns

bankdata.shape
bankdata.info()

bankdata.iloc[:,6]
bankdata.duplicated().sum()
bankdata=bankdata.dropna() ##droping null values since we can't do imputation with text data
bankdata.isna().sum()

bankdata.shape

bankissue=bankdata.Issue
bankissue  ##this is what we require to focus on

bankdata.Product.value_counts()

bankdata['Sub-product'].value_counts()

bankissue

import nltk,spacy,wordcloud,textblob

bankissue=pd.DataFrame(bankissue)
bankdata['Issue'].str.len().hist()   ##from histogram we can see issue column having most characters between 24-26 or 37-40

bankdata['Issue'].str.split().map(lambda x:len(x)).hist()  ##here we get the no. of words present in issue columns in each row.Mostly 3 to 7 words are present.5 to 6 words being the maximum

bankdata['Issue'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist()  ##avg length of words lying between 2 to 9 and 7 being the length of most words

nltk.download('stopwords')
from nltk.corpus import stopwords
stop=set(stopwords.words('english'))    #we have created a set of english stopwords

stop

bankdata['Sub-product'].unique()

##lowercasing issue columns data
bankdata['lowercaseissue']=bankdata['Issue'].apply(lambda x:x.lower())
bankdata['lowercaseissue']

##removing digits and words containing digits,as words containing digits do not give much importance to the main words,regular expression is used
import re
bankdata['digitremoveissue']=bankdata['lowercaseissue'].apply(lambda x: re.sub('\w*\d\w*','', x))
bankdata['digitremoveissue']

bankdata['punctuationissue']=bankdata['digitremoveissue'].apply(lambda x:re.sub("[^A-Za-z" "]+"," ", x))  #removing all unwanted symbols
bankdata['punctuationissue']

##Now we do EDA with cleaned data
#we will do Stop words removal,lemmatisation,Term document matrix
#stopwords removaland lemmatization using spacy
import spacy
nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])

# Lemmatization with stopwords removal
bankdata['lemmatized']=bankdata['punctuationissue'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))
bankdata['lemmatized']
#we have done lemmatization and stopwords removal using spacy.Spacy computes faster than NLTK therefore we use it

#I am adding the length of the cleaned issue and the word count of each cleaned issue
bankdata['cleanIssue_length']=bankdata['lemmatized'].astype(str).apply(len)
bankdata['wordcount']=bankdata['lemmatized'].apply(lambda x:len(str(x).split()))

#lets do polarity sentiment analysis
from textblob import TextBlob
bankdata['polarityscore']=bankdata['lemmatized'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)
bankdata['polarityscore']

bankdata.head()

#lets see the distribution of the word_count, cleanedissue_len, and polarityscore
bankdata[['cleanIssue_length','wordcount','polarityscore']].hist(bins=20,figsize=(15,10))  ##of the cleaned data issue length is lying betwwen 15 to 35 characters,4 words are present in issue columns most of the times and all the words are neutral having 0 polarity score

bankgrouped=bankdata[['Sub-product','lemmatized']].groupby(by='Sub-product').agg(lambda x:' '.join(x))
bankgrouped

#now its time to create document term matrix
from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(analyzer='word')
data=cv.fit_transform(bankgrouped['lemmatized'])
dtm=pd.DataFrame(data.toarray(),columns=cv.get_feature_names())
dtm.index=bankgrouped.index
dtm
#all steps before doing eda is finished
#from document term matrix we can see for subproduct Checking account,terms like account,closing,management,opening are used 21520 times

#now its time to do EDA,we will be knowing a lot about data
#we will be seeing common words from document term matrix using wordcloud
from wordcloud import WordCloud  #to see the words and their size as per the frequency of its occurance
from textwrap import wrap  #textwrap used for wrapping longer text
print(data)
print(bankdata['lemmatized'])
tdm=dtm.transpose()
print(tdm)  #transpose of doucument term matrix

#The mapping from textual data to real valued vectors is called feature extraction,we will do feature extraction in terms of BOW representation where we can represent each sentence or document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary.
#We will apply the following steps to generate our model.

#We declare a dictionary to hold our bag of words.
#Next we tokenize each sentence to words.
#Now for each word in sentence, we check if the word exists in our dictionary.
#If it does, then we increment its count by 1. If it doesnâ€™t, we add it to our dictionary and set its count as 1
import nltk
nltk.download('punkt')
word2count = {}
for data in bankdata['lemmatized']:
    words = nltk.word_tokenize(data)
    for word in words:
        if word not in word2count.keys():
            word2count[word] = 1
        else:
            word2count[word] += 1
word2count
##top 10 words that are mostly repeated are debit,atm,card,account,opening,closing,management,deposit,withdrawal,problem

#we select a particular number of most frequently used words. To implement this we use
import heapq   #It is very useful is implementing priority queues where the queue item with higher weight is given more priority in processing
freq_words = heapq.nlargest(5, word2count, key=word2count.get)
freq_words  ##account,closing,opening,management,deposit there are based on the priorities of repeatation #top5

#In this step we construct a vector, which would tell us whether a word in each sentence is a frequent word or not. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0
X = []
for data in bankdata['lemmatized']:
    vector = []
    for word in freq_words:
        if word in nltk.word_tokenize(data):
            vector.append(1)
        else:
            vector.append(0)
    X.append(vector)
X = np.asarray(X)
X

wc=WordCloud(width=1500,height=1000,max_words=200,stopwords=stop,colormap='Dark2').generate(str(bankdata['lemmatized']))
plt.figure(figsize=(15,10))
plt.imshow(wc.recolor(colormap= 'viridis' , random_state=42), alpha=0.98)
plt.title("frequent words used in issues")
plt.axis("off")
plt.show()  #deposit, withdrawl,account,management,billing,opening,customer,dispute etc words are used a lot of times.This clearly tells that customers are having issues with account opening,account management,deposits,withdrawal.

#lets do bigram analysis
text=" ".join(issue for issue in bankdata['lemmatized'])
text

import nltk
nltk.download('punkt')
nltk_tokens = nltk.word_tokenize(text)  
bigrams_list = list(nltk.bigrams(nltk_tokens))
bigrams_list  ##this is giving us bigram analysis

dictionary2 = [' '.join(tup) for tup in bigrams_list]
dictionary2 #we concatinated the string in list

vectorizer = CountVectorizer(ngram_range=(2, 2))

bag_of_words = vectorizer.fit_transform(dictionary2)
vectorizer.vocabulary_

sum_words = bag_of_words.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
print(words_freq[:100])

# Generating wordcloud
words_dict = dict(words_freq)
WC_height = 1000
WC_width = 1500
WC_max_words = 200
wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width, stopwords=stop)
wordCloud.generate_from_frequencies(words_dict)

plt.figure(4)
plt.title('Most frequently occurring bigrams connected by same colour and font size')
plt.imshow(wordCloud, interpolation='bilinear')
plt.axis("off")
plt.show()

#we can see most frequently occuring bigrams are deposit withdrawal,opening closing,closing management,account opening.So most of the problems are based on accounts and deposits

#lets do trigram analysis to know more
trigrams_tokens = nltk.word_tokenize(text)  
trigrams_list = list(nltk.trigrams(trigrams_tokens))
trigrams_list

dictionary3=[" ".join(tup) for tup in trigrams_list]
dictionary3

vectorizer = CountVectorizer(ngram_range=(3, 3))

bag_of_words1 = vectorizer.fit_transform(dictionary3)
vectorizer.vocabulary_

sum_words1 = bag_of_words1.sum(axis=0)
words_freq1 = [(word, sum_words1[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq1 =sorted(words_freq1, key = lambda x: x[1], reverse=True)
print(words_freq1[:100])

# Generating wordcloud
words_dict1 = dict(words_freq1)
WC_height = 1000
WC_width = 1500
WC_max_words = 200
wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width, stopwords=stop)
wordCloud.generate_from_frequencies(words_dict1)

plt.figure(4)
plt.title('Most frequently occurring trigrams connected by same colour and font size')
plt.imshow(wordCloud, interpolation='bilinear')
plt.axis("off")
plt.show()

#most frequent trigram words are management account opening,closing management acount,account opening closing,problem cause fund,make receive payment etc

#MODEL BUILDING
#lets do TFIDF vectorizer where text data are converted to number format with some weighted information
## sublinear_df is set to True to use a logarithmic form for frequency
# min_df is the minimum numbers of documents a word must be present in to be kept
# norm is set to l2, to ensure all our feature vectors have a euclidian norm of 1
# ngram_range is set to (1, 2) to indicate that we want to consider both unigrams and bigrams
# stop_words is set to "english" to remove all common pronouns ("a", "the", ...) to reduce the number of noisy features
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer=TfidfVectorizer(sublinear_tf=True,norm='l2',min_df=10,ngram_range=(1,2),stop_words='english')
x_train_vc=vectorizer.fit_transform(bankdata['lemmatized'])
pd.DataFrame(x_train_vc.toarray(), columns=vectorizer.get_feature_names()).head()

#KMeans clustering
from sklearn.cluster import KMeans
clus=[]
k_clus=10
for i in range(1,k_clus+1):
  kmeans=KMeans(n_clusters=i,init='k-means++',n_init=5,random_state=0)
  kmeans.fit_transform(x_train_vc)
  clus.append(kmeans.inertia_)
plt.plot(range(1,k_clus + 1 ),clus)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('clus')
plt.savefig('elbow.png')
plt.show()   #from the elbow curve we get k=5 as ideal no. of clusters

model=KMeans(n_clusters=5,init='k-means++',n_init=5,random_state=0)
model.fit(x_train_vc)
clusters=model.predict(x_train_vc)
bankdata['clusters']=clusters
bankdata.head()

def get_top_features_cluster(tf_idf_array, prediction, n_feats):
    labels = np.unique(prediction)
    dfs = []
    for label in labels:
        id_temp = np.where(prediction==label) # indices for each cluster
        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster
        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores
        features = vectorizer.get_feature_names()
        best_features = [(features[i], x_means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['features', 'score'])
        dfs.append(df)
    return dfs


def plotWords(dfs, n_feats):
    for i in range(0, len(dfs)):
        plt.figure(figsize=(8, 2))
        plt.title(("Most Common Words in Cluster {}".format(i)), fontsize=10, fontweight='bold')
        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])

dfs = get_top_features_cluster(x_train_vc.toarray(), clusters, 4)
plotWords(dfs, 4)
#we are plotting bar graph as per the clusters,we found out all in cluster 0 most common words are account opening,opening closing,opening,management
#withdrawal,deposit,deposit withdrawal are mostly repeated in cluster 1
#debit,debit atm,atm,atm card in cluster 2
#low,cause,fundlow,fund in cluster 3
#payment,send money,send,recieved in cluster 4
#as we can see cluster 1 is having issues of accounts,cluster 2 is having issues of withdrawal,cluster 3 is having issues of atm cards,cluster 4 is having issues of funds and cluster 5 is having issues of transfers

#naming particular clusters based on their respective issues
bankdata['clusters']=np.where(bankdata['clusters']=='0','account_management',bankdata['clusters'])
bankdata['clusters']=np.where(bankdata['clusters']=='1','withdrawal_and_deposit',bankdata['clusters'])
bankdata['clusters']=np.where(bankdata['clusters']=='2','debitcard_creditcard',bankdata['clusters'])
bankdata['clusters']=np.where(bankdata['clusters']=='3','funds',bankdata['clusters'])
bankdata['clusters']=np.where(bankdata['clusters']=='4','transactions',bankdata['clusters'])
bankdata.head()

from sklearn.decomposition import PCA

sklearn_pca = PCA(n_components = 2)
Y_sklearn = sklearn_pca.fit_transform(x_train_vc.toarray())
kmeans = KMeans(n_clusters=5, max_iter=600, algorithm = 'auto')
fitted = kmeans.fit(Y_sklearn)
prediction = kmeans.predict(Y_sklearn)

plt.figure(figsize=(14, 7))
plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=prediction, s=40, cmap='viridis', linewidths=5)

centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='black', s=200, alpha=0.6);

#we found out clusters are having large distance between each other,which is good

#we we will be using different supervised models to predict clusters with input as issues
#we would be checking models like:Naive bayes,random forest,logistic regression,support vector machine
#MULTINOMIAL NB
x=x_train_vc
y=bankdata['clusters']
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

modelNB=MultinomialNB()
modelNB.fit(x_train,y_train)
predicttrainNB=modelNB.predict(x_train)
predicttrainNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
train_accuNB=accuracy_score(y_train,predicttrainNB)
predicttestNB=modelNB.predict(x_test)
test_accuNB=accuracy_score(y_test,predicttestNB)
print("train accuracy and test accuracy are ", (train_accuNB,test_accuNB))  ##train accuracy 99.842% and test accuracy 99.809%

confusion_matrix(predicttrainNB,y_train) #106 errors are there rest all are accurate

confusion_matrix(predicttestNB,y_test)  #32 errors are there rest all are accurate

from sklearn.linear_model import LogisticRegression
modellogistic=LogisticRegression()
modellogistic.fit(x_train,y_train)
predtrainlog=modellogistic.predict(x_train)
acculog1=accuracy_score(predtrainlog,y_train)
predtestlog=modellogistic.predict(x_test)
acculog2=accuracy_score(predtestlog,y_test)
print(acculog1,acculog2)
#clearly it is overfitting

from sklearn.svm import SVC
modelsvm=SVC()
modelsvm.fit(x_train,y_train)
predtrainsvm=modelsvm.predict(x_train)
accusvm1=accuracy_score(predtrainsvm,y_train)
predtestsvm=modelsvm.predict(x_test)
accusvm2=accuracy_score(predtestsvm,y_test)
print(accusvm1,accusvm2)
#it is also overfitting

modelrandom=RandomForestClassifier(n_estimators=200,criterion="entropy",min_samples_leaf=50,min_samples_split=50)
modelrandom.fit(x_train,y_train)
predtrainrandom=modelrandom.predict(x_train)
accurandom1=accuracy_score(predtrainrandom,y_train)
predtestrandom=modelrandom.predict(x_test)
accurandom2=accuracy_score(predtestrandom,y_test)
print(accurandom1,accurandom2)

from sklearn.naive_bayes import GaussianNB
modelgaus=GaussianNB()
modelgaus.fit(x_train.todense(),y_train)
predtraingaus=modelgaus.predict(x_train.todense())
accugaus1=accuracy_score(predtraingaus,y_train)
predtestgaus=modelgaus.predict(x_test.todense())
accugaus2=accuracy_score(y_test,predtestgaus)
print(accugaus1,accugaus2)
#same overfitting problem existing


#thus multinomial naive bayes is giving right fit model
#we are going with that

import pickle
#saving model in the disk
pickle.dump(modelNB,open('finalmodel.pkl','wb'))

#loading model 
mymodel=pickle.load(open('finalmodel.pkl','rb'))
pickle.dump(vectorizer,open('vector.pkl','wb'))
tfidfmodel=pickle.load(open('vector.pkl','rb'))

from google.colab import files
files.download('finalmodel.pkl')
files.download('vector.pkl')